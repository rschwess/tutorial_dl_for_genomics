{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning for Genomics Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use a convolutional neuronal network to address a fairly basic but common problem in genomics. Given a set of sequences belonging to different classes, what are the characteristics in the DNA sequence that let us distinguish the classes. For example, given a set of promoter and enhancer sequences, we could ask if there are any patterns in the DNA that let us distinguish between the two. \n",
    "\n",
    "For a 'simple' example, think of two ChIP-seq experiments for two different transcription factors. After analysing the ChIP-seq data, we know at what positions in the genome the two factors bind and the majority of binding sites might be distinct. If we extract the underlying sequences and search for DNA patterns that are enriched in the respective sets, we get an idea what DNA sequences the transcription factor might bind and/or which co-factors influence their binding. \n",
    "\n",
    "Traditionally, people use motif discovery tools for finding overrepresented words and motifs. However, if we move to slightly more complicated questions these methods quickly reach their limits and machine learning approaches become more promising.\n",
    "\n",
    "A more complicated question: If we have multiple sets of enhancers that are active in different tissues and cell types and we have the underlying sequences, can we figure out what sequence patterns are characteristic for what activity? And once we know that can we infer which factors are common and which are tissue specific?\n",
    "\n",
    "------\n",
    "\n",
    "For our test dataset we have a simplfied, simulated version of such a task. We simulated 40,000 DNA sequences of length 200 bp. We split them into 4 enhancer classes and populated them with transcription factor binding motifs and other DNA patters to make them distinguishable. However, some motifs are shared between classes, they may overlap each other and are not necessarily perfect matches to the text book motifs. Thats much more how regulatory DNA actually looks like :)!\n",
    "\n",
    "We will use keras to build and train a small convolutional neuronal network to classify our enhancer sequences. Once this network is trained well, we can than investigate how the network has learned to distinguish between the classes and try to relate this back to transcriptions factor motifs and so on.\n",
    "\n",
    "You can either run everything in an interactive python or ipython session or (especially later when optimizing) just adjust and run python dl_intro.py in the terminal.\n",
    "\n",
    "-----\n",
    "\n",
    "# Data Set Up and Import\n",
    "\n",
    "Lets start by looking at the data. We have 40,000 sequences and they are all labeled with their respective class. We already split them up into taining, test and validation set. It is also a good idea to check if our classes are roughly equally distributed across our different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers:\n",
      "   1000 ./data/pwm_seq_200bp_test_set.txt\n",
      "  38000 ./data/pwm_seq_200bp_train_set.txt\n",
      "   1000 ./data/pwm_seq_200bp_valid_set.txt\n",
      "  40000 total\n",
      "\n",
      "Format\n",
      "3\tATGGCTGATAATGACGATTGTACAGATGGTGGATGAGATTGCCTCGTCCCGGCAGCATTACCCCCTGGTGGCAACGGCCACCAGGGGGCAATAAATCTGTGTCTTATCTCCGAGACCAAACAATTCCACAGCCTCTTATACAGCACCGAATGGACCGCCCCCTGGTGGCCAGGTATCGTCGAGGGCTCAATTAAACTCCT\n",
      "1\tGCAGGCATTATGAGGTAATAAACTCAGCGCGTGTTGAGATAAGATTCTAAGCGGCGCGCGCGCGCGACCGCGAGAAGTGGAGATTAAGCGCGCTAATGGTGTGTCCGATAGTCACGTGTCCGCGCGGCGCGCGCCATGTATGTTCTGTTCTGCGCGCCGCGCTTTGCGCGCGCGCTTGGTATATAAAGCTGGGTTTTAAT\n",
      "1\tGGCGCGCCTGGCATTTCTTAGAGAGGCGCGCAATACAACGAGAATCACCTAGAAGCCGTGTCTGTTGCTTATCACCGTTCGCCTAGGCCGCACGGGCACGTGGGTCTCCCGTTCCCTCAATCCTAACAGAAGCGCGCTAAGTCGTCGTTGGCTCTCTTACTAGCAGCGCGCCTGTACTAACCCGGCACTCGGCGGTGGGC\n",
      "\n",
      "Class Representations:\n",
      "\n",
      "Training:\n",
      "   9489 0\n",
      "   9513 1\n",
      "   9508 2\n",
      "   9490 3\n",
      "\n",
      "Test:\n",
      "    268 0\n",
      "    237 1\n",
      "    243 2\n",
      "    252 3\n",
      "\n",
      "Validation:\n",
      "    243 0\n",
      "    250 1\n",
      "    249 2\n",
      "    258 3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# check lines per set\n",
    "echo \"Numbers:\"\n",
    "wc -l ./data/pwm*\n",
    "\n",
    "# check data format\n",
    "echo -ne \"\\nFormat\\n\"\n",
    "head -n 3 ./data/pwm_seq_200bp_test_set.txt\n",
    "\n",
    "# check class representation\n",
    "echo -ne \"\\nClass Representations:\\n\"\n",
    "\n",
    "echo -ne \"\\nTraining:\\n\"\n",
    "cut -f 1 ./data/pwm_seq_200bp_train_set.txt | sort | uniq -c\n",
    "echo -ne \"\\nTest:\\n\"\n",
    "cut -f 1 ./data/pwm_seq_200bp_test_set.txt | sort | uniq -c\n",
    "echo -ne \"\\nValidation:\\n\"\n",
    "cut -f 1 ./data/pwm_seq_200bp_valid_set.txt | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Looks good, lets move straight in! We will use keras with tensorflow as its backend. Keras is ideal for quickly writing down and prototyping networks in just a few lines of code. The documentation site will be usefull throught the tutorial https://keras.io/.\n",
    "\n",
    "We import keras and the relevant layers and operations we need. (Don't worry about the anoying warning if you run under python 3.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote two helper functions to convert the sequence into a hot encoded sequences and a wrapper to read in and assemble the data. Feel free to skip over this but you might want to have a quick look and understand how we format the data. The hot encoding transforms the sequence into a X x 4 array whith rows corresponding to the sequence position and the columns representing the 4 DNA bases. The respective base column that matches the sequences at that position is 1 the rest 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Function  get hotcoded sequence\n",
    "def get_hot_coded_seq(sequence):\n",
    "    \"\"\"Convert a 4 base letter sequence to 4-row x-cols hot coded sequence\"\"\"\n",
    "    # initialise empty\n",
    "    hotsequence = np.zeros((len(sequence),4))\n",
    "    # set hot code 1 according to gathered sequence\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i] == 'A':\n",
    "            hotsequence[i,0] = 1\n",
    "        elif sequence[i] == 'C':\n",
    "            hotsequence[i,1] = 1\n",
    "        elif sequence[i] == 'G':\n",
    "            hotsequence[i,2] = 1\n",
    "        elif sequence[i] == 'T':\n",
    "            hotsequence[i,3] = 1\n",
    "    # return the numpy array\n",
    "    return hotsequence\n",
    "\n",
    "# Helper function to read in the labels and seqs and store as hot encoded np array\n",
    "def read_data(infile):\n",
    "    # read file in\n",
    "    with open(infile, \"r\") as f:\n",
    "        seqs = []\n",
    "        labels = []\n",
    "        for i,l in enumerate(f):\n",
    "            l = l.rstrip()\n",
    "            l = l.split(\"\\t\")\n",
    "            seqs.append(l[1])\n",
    "            labels.append(l[0])\n",
    "    # make labels np.array\n",
    "    labels = np.array(labels)\n",
    "    # convert to one_hot_labels\n",
    "    hot_labels = keras.utils.to_categorical(labels, num_classes=4)\n",
    "    # make seqs np.array\n",
    "    hot_seqs = np.zeros( (len(seqs), 200, 4) )\n",
    "    # fill with hot encoded sequences\n",
    "    for j in range(len(seqs)):\n",
    "        hotsequence = get_hot_coded_seq(seqs[j])\n",
    "        hot_seqs[j,] = hotsequence\n",
    "    return hot_labels, hot_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data --------------------------------------------------------------------\n",
    "train_file = \"./data/pwm_seq_200bp_train_set.txt\"\n",
    "train_labels, train_seqs = read_data(train_file)\n",
    "test_file = \"./data/pwm_seq_200bp_test_set.txt\"\n",
    "test_labels, test_seqs = read_data(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check how the data look like after we read and hot encoded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Seq Shape (38000, 200, 4)\n",
      "Train Label Shape (38000, 4)\n",
      "Labels Format:\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "Seq Format (first 10 bp):\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# check shapes\n",
    "print(\"Train Seq Shape\", train_seqs.shape)\n",
    "print(\"Train Label Shape\", train_labels.shape)\n",
    "\n",
    "# check data format\n",
    "print(\"Labels Format:\")\n",
    "print(train_labels[1:5])\n",
    "print(\"Seq Format (first 10 bp):\")\n",
    "print(train_seqs[1, 1:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels have a one hot encoding where every column represents a different class and in this case only one class can be active at a time.\n",
    "\n",
    "The sequences have shape [sample x sequence_length x basis]. As a comparison a set of 2D images would have the dimensions [sample x pixel_rows x pixel_columns x colour_channel]. A grey scale picture would have only one channel while RGB images have three. We can thus think of our sequence as a 1D image with 4 channels.\n",
    "\n",
    "----\n",
    "\n",
    "# Building the Network\n",
    "\n",
    "We now define our Network. We first set some global and network architechture options and put them all together in the keras Sequqnetial mode. The sequential mode is an easy wrapper for linearly stacked networks that makes your code even more concise. We just define the model to be sequential and than add/stack layer after layer. Here we use a simple convolutional architecture. \n",
    "\n",
    "* Our first layer is a 1D convolution over the input:\n",
    "   * We use a 1D convolution because we only want the filter to move along the sequence axis and map the channels to the hidden units.\n",
    "   * We start with 10 hidden units or filters or kernels which are all of length 10 (bp)\n",
    "   * We use the RELU activation function\n",
    "   * We also define the input shape and how to pad the input if necessary (see doc.)\n",
    "* We next perform max pooling where we take the maximum of a window of 5 consecutive activation values\n",
    "    * This reduces the data dimension, thus simplifying the model and speeding up further computations\n",
    "    * But it also enforces some extend of positional invariance into our model. For example, if we have a match to transcription factor motif in our sequence, we don't necessarily care where exactly this mtif lies and a few bp up- or downstream shouldn't make a difference to our predictions.\n",
    "* We then \"Flatten\" the activation values to a 1 dimensional vector\n",
    "* And apply a fully connected or \"Dense\" layer connecting every value in the 1D vector to every class prediction\n",
    "    * we use sigmoid (softmax) activation function to perform effectively a multinomial logistic regression \n",
    "\n",
    "The number of hidden units, the size of the kernel, the pooling size, but also the number and types of layers we use in the network are usually called hyperparameters. The most work in DL usually comes down to finding the right hyperparameters that let our network training converge and that give us the best possible (at least the best we are able to find) accuracies.\n",
    "\n",
    "We set some reasonable choices to begin with. But your task will be to play with these hyperparameters and see how well you can tune the model with a few adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global options\n",
    "num_classes = 4\n",
    "\n",
    "# network architecture options\n",
    "conv1_hidden_units = 10\n",
    "conv1_filter_size = 5\n",
    "maxpool1_width = 5\n",
    "\n",
    "# construct the model ----------------------------------------------------------\n",
    "model = Sequential()\n",
    "model.add(Conv1D(conv1_hidden_units, kernel_size=(conv1_filter_size), activation='relu', input_shape=(200, 4), padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=maxpool1_width))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile the model. We use adam as our optimizer. Since the classes are mutually exclusive we select the binary_crossentropy as our loss function and we want to monitor the accuracy during training.\n",
    "\n",
    "We also print a summary of our network telling us the data shapes throught the network and sumarizing the number of trainable parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 200, 10)           210       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 40, 10)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1604      \n",
      "=================================================================\n",
      "Total params: 1,814\n",
      "Trainable params: 1,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the fully connected layer has in comparison way more parameters to the convolutional layer.\n",
    "\n",
    "-----\n",
    "\n",
    "# Training\n",
    "\n",
    "Now that we have our model or graph set up we can train it. We feed the model with our training sequences and labels, we define a batch size (since we are training in batch mode) and set the number of epochs (cycles through the training data) we want to train for. Five epochs should be fine for us feel free to ramp this up a bit and see if you get improvements or if the learning plateus quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "38000/38000 [==============================] - 2s 63us/step - loss: 0.3630 - acc: 0.8311 - val_loss: 0.2440 - val_acc: 0.9012\n",
      "Epoch 2/5\n",
      "38000/38000 [==============================] - 2s 59us/step - loss: 0.2070 - acc: 0.9178 - val_loss: 0.1795 - val_acc: 0.9315\n",
      "Epoch 3/5\n",
      "38000/38000 [==============================] - 2s 61us/step - loss: 0.1648 - acc: 0.9330 - val_loss: 0.1543 - val_acc: 0.9373\n",
      "Epoch 4/5\n",
      "38000/38000 [==============================] - 2s 64us/step - loss: 0.1429 - acc: 0.9419 - val_loss: 0.1360 - val_acc: 0.9417\n",
      "Epoch 5/5\n",
      "38000/38000 [==============================] - 3s 67us/step - loss: 0.1278 - acc: 0.9482 - val_loss: 0.1275 - val_acc: 0.9458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdb0beddf98>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Options\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "\n",
    "# Train ------------------------------------------------------------------------\n",
    "model.fit(train_seqs, train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(test_seqs, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks alright, the training as well as the test accuracy is climbing from epoch to epoch and slows down a little more after every epoch. It is now your task to find better hyperparameters for our network and training procedure to see how high up you can get the accuracy. For doing that, I suggest taking dl_intro.py, commenting out all the code after the training, adjust your hyperparamters and/or network architectures how you like and running it in the terminal via python dl_intro.py. \n",
    "\n",
    "Tipps: \n",
    "\n",
    "* Should we train longer?\n",
    "\n",
    "* Do we need more hidden layers?\n",
    "\n",
    "* Could we max pool over the entire sequence to get one output per filter?\n",
    "\n",
    "* Would a second layer be beneficial?\n",
    "\n",
    "* You could bias the network architechture with some biological knowledge: How long are transcription factor binding motifs in general and what would be an appropriate filter_width then?\n",
    "\n",
    "* (Hint: you can do pretty well in 5 - 10 epochs with minor tweaks!)\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "# Evaluation and Prediction\n",
    "\n",
    "Once you are happy with you network performance or in case you want to jump ahead first and optimize later, we will evaluate our network on the held out validation data. Technically, we only optimized on the training data set but we always kept an eye on the test data loss as well. We discarding a nets that does well on the training but worse at the test (overfitted), therefore we always have an intrinsic bias. The validation data set is meant to have never been touched throught the whole optimization process and we evaluate the perormance of our final model on this set to get an unbiased estimate of its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.127488910317\n",
      "Test accuracy: 0.94575\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ---------------------------------------------------------------------\n",
    "valid_file = \"./data/pwm_seq_200bp_valid_set.txt\"\n",
    "valid_labels, valid_seqs = read_data(test_file)\n",
    "score = model.evaluate(valid_seqs, valid_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with our network we obviously want to employ it as well. Lets say we have a new sequence we want to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: CATCGTCATATATGTAGTAACCATCCTGTATATATGCCCTCTGGTATATATTGATATATGACCGCCACCTGTTGGCAATATAAAGAGATCTAGTATGCAAGAAGCCCACGACCGAAGTCTCCGCCAGTAGGGGTCAGCCACAAGGGGGCGCTATAGGCGCAATTGCGATACTATATATTACAGCACATGACCACGCGACG\n",
      "\n",
      "Class Prediction \"Probability\":\n",
      "\tClass 0 = 1.11973e-07\n",
      "\tClass 1 = 9.73746e-06\n",
      "\tClass 2 = 0.999906\n",
      "\tClass 3 = 8.44383e-05\n",
      "\n",
      "True Class: 2\n"
     ]
    }
   ],
   "source": [
    "# Predictions ------------------------------------------------------------------\n",
    "# read valid sequences again\n",
    "with open(valid_file, \"r\") as f:\n",
    "    seqs = []\n",
    "    labels = []\n",
    "    for i,l in enumerate(f):\n",
    "        l = l.rstrip()\n",
    "        l = l.split(\"\\t\")\n",
    "        seqs.append(l[1])\n",
    "        labels.append(l[0])\n",
    "\n",
    "# select a single sequence\n",
    "single_seq = seqs[0]\n",
    "single_label = labels[0]\n",
    "\n",
    "print(\"Sequence: \" + single_seq)\n",
    "\n",
    "# hot encode        \n",
    "hotseq = get_hot_coded_seq(single_seq)\n",
    "\n",
    "# calculate predictions\n",
    "single_prediction = model.predict(np.expand_dims(hotseq, axis=0))\n",
    "print(\"\\nClass Prediction \\\"Probability\\\":\")\n",
    "print(\"\\tClass 0 = %s\" % single_prediction[0][0])\n",
    "print(\"\\tClass 1 = %s\" % single_prediction[0][1])\n",
    "print(\"\\tClass 2 = %s\" % single_prediction[0][2])\n",
    "print(\"\\tClass 3 = %s\" % single_prediction[0][3])\n",
    "\n",
    "print(\"\\nTrue Class: \" + single_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "[[  1.22680882e-04   1.51699204e-02   9.16516483e-01   6.81908280e-02]\n",
      " [  2.13671137e-06   6.69176748e-04   8.97344708e-01   1.01984017e-01]\n",
      " [  7.87706971e-01   2.12291107e-01   8.35260380e-07   1.05343008e-06]]\n"
     ]
    }
   ],
   "source": [
    "# or just run all predictions for \n",
    "all_valid_predictions = model.predict(valid_seqs)\n",
    "print(all_valid_predictions.shape)\n",
    "print(all_valid_predictions[5:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Inspection\n",
    "\n",
    "Now that we have a reasonably working model, we also want to inspect and see what the net has learned. In applications we often don't care what the network has learned as long as it performs well and outperforms our compatitors. For many problems however, we are exactly interested in what the network has learned. What features distinguish a cat from a dog or if it comes to decision making (e.g. health care or self driving cars, we obviously want to be able to understand and be able to justify why a certain decision has been chosen and learn how to correct missbehavior.\n",
    "\n",
    "In genomics we usually want to learn what sequence features distinguish the sequences from one another and map them back to biological properties and factors. The easiest way is to just plot the filter weights. In the first convolutional layer, our filters are just like position weight matrices, multiplying every base at every position with a learned weight and summing the value up (plus a bias and pipe it through the RELU activation function). Unfortunatly, this becomes less straight forward to interpret in deeper layers. There are ways of back engineering and learning the importance of filters in higher layers (e.g. https://github.com/kundajelab/deeplift) but we concern ourself only with the simple first layer here.\n",
    "\n",
    "We can get the weigths of the filters from the model, save them as .txt files and plot them out. I wrote aa wrapper to plot the filter weigths for you in R. Run the code, check the filter_X.txt filtes and look at the plots and try to interpret them.\n",
    "\n",
    "* Do any look like transciption factor binding sites you know?\n",
    "* Do you recognize any sequence features that are not binding motifs?\n",
    "* Can you simpliy the sequences/ motifs from the plot an query them in a transcription factor binding motif database (http://jaspar.genereg.net/)\n",
    "* What is your best bet: Which sequence motifs did we use for simulating the sequence classes?\n",
    "* Check the input data. Split them up by class into text files with only the sequences one sequence per line (see example). Query them in standard motif analysis tools (e.g. http://rsat.sb-roscoff.fr/oligo-analysis_form.cgi or http://meme-suite.org/tools/meme). Do these tools find different or similar things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect weights --------------------------------------------------------------\n",
    "model_weights = model.get_weights()\n",
    "filter_weights = model_weights[0]\n",
    "\n",
    "# save conv filter weights\n",
    "for k in range(model_weights[0].shape[2]):\n",
    "    # save single filter weights\n",
    "    np.savetxt((\"./visualize/filter_%s.txt\" % k), filter_weights[:,:,k], delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] \"filter_0.txt\" \"filter_1.txt\" \"filter_2.txt\" \"filter_3.txt\" \"filter_4.txt\"\n",
      " [6] \"filter_5.txt\" \"filter_6.txt\" \"filter_7.txt\" \"filter_8.txt\" \"filter_9.txt\"\n",
      "[1] \"Saving Plot filter_0.png\"\n",
      "[1] \"Saving Plot filter_1.png\"\n",
      "[1] \"Saving Plot filter_2.png\"\n",
      "[1] \"Saving Plot filter_3.png\"\n",
      "[1] \"Saving Plot filter_4.png\"\n",
      "[1] \"Saving Plot filter_5.png\"\n",
      "[1] \"Saving Plot filter_6.png\"\n",
      "[1] \"Saving Plot filter_7.png\"\n",
      "[1] \"Saving Plot filter_8.png\"\n",
      "[1] \"Saving Plot filter_9.png\"\n",
      "[1] \"Passing: 10  Skipped: 0\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Plot them using the supplied R script\n",
    "Rscript ./helper/plot_sequence_kernel_weights_per_dir.R ./visualize ./visualize plot_weight 10 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl_tutorial]",
   "language": "python",
   "name": "conda-env-dl_tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
